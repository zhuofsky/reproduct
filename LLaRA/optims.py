import math

class LinearWarmupCosineLRScheduler:
    def __init__(self,
                 optimizer,
                 min_lr,
                 init_lr,
                 warmup_steps=0,
                 warmup_start_lr=-1,
                 **kwargs):
        self.optimizer = optimizer
        self.min_lr = min_lr  # min_lr是最后保持的学习率
        self.init_lr = init_lr  # 这个是最高点的学习率
        self.warmup_steps = warmup_steps  # 热启动
        self.warmup_start_lr = warmup_start_lr if warmup_start_lr >= 0 else init_lr  # 热启动的初始学习率

    def step(self, cur_step, cur_epoch, max_step):
        if cur_epoch == 0 and cur_step < self.warmup_steps:
            warmup_lr_schedule(
                step=cur_step,
                optimizer=self.optimizer,
                max_step=self.warmup_steps,
                init_lr=self.warmup_start_lr,
                max_lr=self.init_lr,
            )
        else:
            cosine_lr_schedule(
                step=cur_step,
                optimizer=self.optimizer,
                max_step=max_step,
                init_lr=self.init_lr,
                min_lr=self.min_lr,
            )

    def state_dict(self):
        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}

    def load_state_dict(self, state_dict):
        self.__dict__.update(state_dict)



def cosine_lr_schedule(optimizer, step, max_step, init_lr, min_lr):
    """Decay the learning rate"""
    lr = (init_lr - min_lr) * 0.5 * (
            1.0 + math.cos(math.pi * step / max_step)
    ) + min_lr
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr

def warmup_lr_schedule(optimizer, step, max_step, init_lr, max_lr):
    """Warmup the learning rate"""
    lr = min(max_lr, init_lr + (max_lr - init_lr) * step / max(max_step, 1))
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr